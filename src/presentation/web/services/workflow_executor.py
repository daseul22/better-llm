"""
ì›Œí¬í”Œë¡œìš° ì‹¤í–‰ ì—”ì§„

ì›Œí¬í”Œë¡œìš°ì˜ ë…¸ë“œë¥¼ ìˆœì°¨ì ìœ¼ë¡œ ì‹¤í–‰í•˜ê³ , ë…¸ë“œ ê°„ ë°ì´í„° ì „ë‹¬ì„ ê´€ë¦¬í•©ë‹ˆë‹¤.
"""

import asyncio
import time
from datetime import datetime
from typing import Dict, Any, AsyncIterator, Set, List, Optional, Tuple
from collections import deque
from dataclasses import replace
from pathlib import Path

from src.domain.models import AgentConfig, Message
from src.infrastructure.config import JsonConfigLoader, get_project_root
from src.infrastructure.claude.worker_client import WorkerAgent
from src.infrastructure.storage.custom_worker_repository import CustomWorkerRepository
from src.infrastructure.logging import get_logger, add_session_file_handlers, remove_session_file_handlers
from src.presentation.web.schemas.workflow import (
    Workflow,
    WorkflowNode,
    WorkflowEdge,
    WorkflowNodeExecutionEvent,
    WorkerNodeData,
    InputNodeData,
    ConditionNodeData,
    MergeNodeData,
    TokenUsage,
)

logger = get_logger(__name__)


def extract_text_from_worker_output(output: str) -> str:
    """
    Worker ì¶œë ¥ì—ì„œ ìµœì¢… í…ìŠ¤íŠ¸ë§Œ ì¶”ì¶œ

    Worker ì¶œë ¥ì€ thinking, tool_use, tool_result, text ë¸”ë¡ì„ í¬í•¨í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
    ì´ í•¨ìˆ˜ëŠ” type="text"ì¸ ë¸”ë¡ë§Œ ì¶”ì¶œí•˜ì—¬ ë°˜í™˜í•©ë‹ˆë‹¤.

    Args:
        output: Workerì˜ ì „ì²´ ì¶œë ¥

    Returns:
        str: ìµœì¢… í…ìŠ¤íŠ¸ë§Œ ì¶”ì¶œëœ ê²°ê³¼
    """
    import json

    text_parts = []

    # {"role": "assistant"ë¡œ ì‹œì‘í•˜ëŠ” JSON ê°ì²´ ì°¾ê¸° (ì¤‘ê´„í˜¸ ì¹´ìš´íŒ…)
    start_pattern = '{"role":'
    idx = 0

    while idx < len(output):
        # {"role": íŒ¨í„´ ì°¾ê¸°
        start_idx = output.find(start_pattern, idx)
        if start_idx == -1:
            break

        # ì¤‘ê´„í˜¸ ì¹´ìš´íŒ…ìœ¼ë¡œ ì™„ì „í•œ JSON ê°ì²´ ì¶”ì¶œ
        brace_count = 0
        in_string = False
        escape_next = False
        end_idx = start_idx

        for i in range(start_idx, len(output)):
            char = output[i]

            if escape_next:
                escape_next = False
                continue

            if char == '\\':
                escape_next = True
                continue

            if char == '"' and not escape_next:
                in_string = not in_string
                continue

            if not in_string:
                if char == '{':
                    brace_count += 1
                elif char == '}':
                    brace_count -= 1
                    if brace_count == 0:
                        end_idx = i + 1
                        break

        # ì™„ì „í•œ JSON ê°ì²´ ì¶”ì¶œ ì‹œë„
        if end_idx > start_idx:
            try:
                json_str = output[start_idx:end_idx]
                data = json.loads(json_str)

                # content ë°°ì—´ì—ì„œ type="text"ì¸ ë¸”ë¡ë§Œ ì¶”ì¶œ
                if isinstance(data.get("content"), list):
                    for block in data["content"]:
                        if isinstance(block, dict) and block.get("type") == "text":
                            text_parts.append(block.get("text", ""))

            except json.JSONDecodeError as e:
                logger.debug(f"JSON íŒŒì‹± ì‹¤íŒ¨ (ìœ„ì¹˜: {start_idx}-{end_idx}): {e}")

            idx = end_idx
        else:
            idx = start_idx + len(start_pattern)

    # í…ìŠ¤íŠ¸ ë¸”ë¡ì„ ì°¾ì•˜ìœ¼ë©´ ì¡°í•©í•˜ì—¬ ë°˜í™˜
    if text_parts:
        result = "\n".join(text_parts).strip()
        logger.debug(f"í…ìŠ¤íŠ¸ ì¶”ì¶œ ì„±ê³µ: {len(text_parts)}ê°œ ë¸”ë¡, {len(result)}ì")
        return result

    # JSON íŒŒì‹± ì‹¤íŒ¨ ì‹œ ì „ì²´ ì¶œë ¥ ë°˜í™˜ (ì•ˆì „ì¥ì¹˜)
    logger.warning("í…ìŠ¤íŠ¸ ë¸”ë¡ì„ ì°¾ì„ ìˆ˜ ì—†ì–´ ì „ì²´ ì¶œë ¥ ë°˜í™˜")
    return output


def classify_chunk_type(chunk: str) -> str:
    """
    Worker ì¶œë ¥ ì²­í¬ì˜ íƒ€ì…ì„ ë¶„ë¥˜í•©ë‹ˆë‹¤.

    Args:
        chunk: ì¶œë ¥ ì²­í¬

    Returns:
        str: "thinking", "tool", "text" ì¤‘ í•˜ë‚˜
    """
    import json

    # JSON ë¸”ë¡ì¸ì§€ í™•ì¸
    if chunk.strip().startswith('{"role":'):
        try:
            data = json.loads(chunk)
            if isinstance(data.get("content"), list):
                for block in data["content"]:
                    if isinstance(block, dict):
                        block_type = block.get("type", "")
                        if block_type in ("thinking", "tool_use", "tool_result"):
                            return "thinking" if block_type == "thinking" else "tool"
                        elif block_type == "text":
                            return "text"
        except json.JSONDecodeError:
            pass

    # ê¸°ë³¸ì ìœ¼ë¡œ textë¡œ ê°„ì£¼
    return "text"


class WorkflowExecutor:
    """
    ì›Œí¬í”Œë¡œìš° ì‹¤í–‰ ì—”ì§„

    ì›Œí¬í”Œë¡œìš°ì˜ ë…¸ë“œë¥¼ ìœ„ìƒ ì •ë ¬í•˜ì—¬ ìˆœì°¨ì ìœ¼ë¡œ ì‹¤í–‰í•˜ê³ ,
    ê° ë…¸ë“œì˜ ì¶œë ¥ì„ ë‹¤ìŒ ë…¸ë“œì˜ ì…ë ¥ìœ¼ë¡œ ì „ë‹¬í•©ë‹ˆë‹¤.

    Attributes:
        config_loader: Agent ì„¤ì • ë¡œë”
        agent_configs: Agent ì„¤ì • ëª©ë¡ (ìºì‹œ)
    """

    def __init__(self, config_loader: JsonConfigLoader, project_path: Optional[str] = None):
        """
        WorkflowExecutor ì´ˆê¸°í™”

        Args:
            config_loader: Agent ì„¤ì • ë¡œë”
            project_path: í”„ë¡œì íŠ¸ ê²½ë¡œ (ì»¤ìŠ¤í…€ ì›Œì»¤ ë¡œë“œìš©, ì˜µì…˜)
        """
        self.config_loader = config_loader
        self.project_path = project_path
        self.agent_configs = config_loader.load_agent_configs()

        # Condition ë…¸ë“œ ë°˜ë³µ íšŸìˆ˜ ì¶”ì  (ì„¸ì…˜ë³„, ë…¸ë“œë³„)
        # {session_id: {node_id: iteration_count}}
        self._condition_iterations: Dict[str, Dict[str, int]] = {}

        # ë…¸ë“œ ì„¸ì…˜ ê´€ë¦¬ (ë…¸ë“œë³„ SDK ì„¸ì…˜ ID ì €ì¥)
        # {node_id: session_id}
        # ë©”ëª¨ë¦¬ ê¸°ë°˜: ì„œë²„ ì¬ì‹œì‘ ì‹œ ì´ˆê¸°í™”
        # ì—¬ëŸ¬ ì›Œí¬í”Œë¡œìš° ì‹¤í–‰ì— ê±¸ì³ ìœ ì§€ë˜ì–´ ì»¨í…ìŠ¤íŠ¸ ì¬í™œìš©
        self._node_sessions: Dict[str, str] = {}

        # ì»¤ìŠ¤í…€ ì›Œì»¤ ë¡œë“œ (í”„ë¡œì íŠ¸ ê²½ë¡œê°€ ì£¼ì–´ì§„ ê²½ìš°)
        self.custom_worker_names = set()
        if project_path:
            try:
                custom_repo = CustomWorkerRepository(Path(project_path))
                custom_workers = custom_repo.load_custom_workers()
                self.agent_configs.extend(custom_workers)
                self.custom_worker_names = {w.name for w in custom_workers}
                logger.info(
                    f"ì»¤ìŠ¤í…€ ì›Œì»¤ ë¡œë“œ ì™„ë£Œ: {len(custom_workers)}ê°œ "
                    f"(í”„ë¡œì íŠ¸: {project_path})"
                )
            except Exception as e:
                logger.warning(
                    f"ì»¤ìŠ¤í…€ ì›Œì»¤ ë¡œë“œ ì‹¤íŒ¨ (í”„ë¡œì íŠ¸: {project_path}): {e}",
                    exc_info=True
                )

        self.agent_config_map = {
            config.name: config for config in self.agent_configs
        }

    def _get_agent_config(self, agent_name: str) -> AgentConfig:
        """
        Agent ì„¤ì • ì¡°íšŒ

        Args:
            agent_name: Agent ì´ë¦„

        Returns:
            AgentConfig: Agent ì„¤ì •

        Raises:
            ValueError: Agentë¥¼ ì°¾ì„ ìˆ˜ ì—†ëŠ” ê²½ìš°
        """
        config = self.agent_config_map.get(agent_name)
        if not config:
            # ë” ëª…í™•í•œ ì—ëŸ¬ ë©”ì‹œì§€ ì œê³µ
            available_agents = list(self.agent_config_map.keys())
            error_msg = (
                f"Agent '{agent_name}'ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\n"
                f"ì‚¬ìš© ê°€ëŠ¥í•œ Agent: {', '.join(available_agents)}"
            )

            # ì»¤ìŠ¤í…€ ì›Œì»¤ì¸ ê²½ìš° ì¶”ê°€ ì•ˆë‚´
            if agent_name not in self.custom_worker_names:
                error_msg += (
                    f"\n\níŒíŠ¸: ê¸°ë³¸ ì œê³µ Workerê°€ ì•„ë‹™ë‹ˆë‹¤. "
                    f"ì»¤ìŠ¤í…€ ì›Œì»¤ì¸ ê²½ìš° í”„ë¡œì íŠ¸ ê²½ë¡œë¥¼ í™•ì¸í•˜ì„¸ìš”."
                )
            else:
                error_msg += (
                    f"\n\níŒíŠ¸: ì»¤ìŠ¤í…€ ì›Œì»¤ '{agent_name}'ê°€ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. "
                    f"í”„ë¡œì íŠ¸ ê²½ë¡œ: {self.project_path}"
                )

            logger.error(error_msg)
            raise ValueError(error_msg)
        return config

    def _topological_sort(
        self, nodes: List[WorkflowNode], edges: List[WorkflowEdge], start_node_id: Optional[str] = None
    ) -> List[WorkflowNode]:
        """
        ì›Œí¬í”Œë¡œìš° ë…¸ë“œ ìœ„ìƒ ì •ë ¬ (Topological Sort)

        Args:
            nodes: ë…¸ë“œ ëª©ë¡
            edges: ì—£ì§€ ëª©ë¡
            start_node_id: ì‹œì‘ ë…¸ë“œ ID (ì˜µì…˜, ì§€ì • ì‹œ í•´ë‹¹ Input ë…¸ë“œë§Œ ì‹œì‘ì ìœ¼ë¡œ ì‚¬ìš©)

        Returns:
            List[WorkflowNode]: ì‹¤í–‰ ìˆœì„œëŒ€ë¡œ ì •ë ¬ëœ ë…¸ë“œ ëª©ë¡

        Raises:
            ValueError: ìˆœí™˜ ì°¸ì¡°ê°€ ìˆëŠ” ê²½ìš°
        """
        # ë…¸ë“œ ID â†’ ë…¸ë“œ ë§¤í•‘
        node_map = {node.id: node for node in nodes}

        # ìœ íš¨í•˜ì§€ ì•Šì€ ì—£ì§€ í•„í„°ë§ (ì¡´ì¬í•˜ì§€ ì•ŠëŠ” ë…¸ë“œë¥¼ ì°¸ì¡°í•˜ëŠ” ì—£ì§€ ì œê±°)
        valid_edges = []
        for edge in edges:
            if edge.source not in node_map:
                logger.warning(
                    f"ì—£ì§€ {edge.id}: source ë…¸ë“œ '{edge.source}'ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ì—£ì§€ë¥¼ ë¬´ì‹œí•©ë‹ˆë‹¤."
                )
                continue
            if edge.target not in node_map:
                logger.warning(
                    f"ì—£ì§€ {edge.id}: target ë…¸ë“œ '{edge.target}'ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ì—£ì§€ë¥¼ ë¬´ì‹œí•©ë‹ˆë‹¤."
                )
                continue
            valid_edges.append(edge)

        # Input ë…¸ë“œ ì°¾ê¸° (ì‹œì‘ì )
        input_nodes = [node for node in nodes if node.type == "input"]
        if not input_nodes:
            raise ValueError("ì›Œí¬í”Œë¡œìš°ì— Input ë…¸ë“œê°€ ì—†ìŠµë‹ˆë‹¤. Input ë…¸ë“œì—ì„œ ì‹œì‘í•´ì•¼ í•©ë‹ˆë‹¤.")

        # start_node_idê°€ ì§€ì •ëœ ê²½ìš° í•´ë‹¹ ë…¸ë“œë§Œ ì‹œì‘ì ìœ¼ë¡œ ì‚¬ìš©
        if start_node_id:
            # ì§€ì •ëœ ë…¸ë“œê°€ Input ë…¸ë“œì¸ì§€ í™•ì¸
            start_node = node_map.get(start_node_id)
            if not start_node:
                raise ValueError(f"ì§€ì •ëœ ì‹œì‘ ë…¸ë“œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {start_node_id}")
            if start_node.type != "input":
                raise ValueError(f"ì‹œì‘ ë…¸ë“œëŠ” Input ë…¸ë“œì—¬ì•¼ í•©ë‹ˆë‹¤: {start_node_id} (íƒ€ì…: {start_node.type})")
            input_node_ids = [start_node_id]
            logger.info(f"íŠ¹ì • Input ë…¸ë“œì—ì„œ ì‹œì‘: {start_node_id}")
        else:
            # start_node_idê°€ ì—†ìœ¼ë©´ ëª¨ë“  Input ë…¸ë“œë¥¼ ì‹œì‘ì ìœ¼ë¡œ ì‚¬ìš© (ê¸°ì¡´ ë™ì‘)
            input_node_ids = [node.id for node in input_nodes]
            logger.info(f"ëª¨ë“  Input ë…¸ë“œì—ì„œ ì‹œì‘: {input_node_ids}")

        # Condition ë…¸ë“œì˜ max_iterationsê°€ ì„¤ì •ëœ ê²½ìš° í”¼ë“œë°± ë£¨í”„ í—ˆìš©
        # ë°±ì—£ì§€(back-edge) ì‹ë³„: Condition ë…¸ë“œì—ì„œ ë‚˜ê°€ëŠ” ì—£ì§€ê°€ ì´ë¯¸ ë°©ë¬¸í•œ ë…¸ë“œë¡œ ê°€ëŠ” ê²½ìš°
        condition_nodes_with_iterations = set()
        for node in nodes:
            if node.type == "condition":
                max_iterations = None
                if hasattr(node.data, "max_iterations"):
                    max_iterations = node.data.max_iterations
                elif isinstance(node.data, dict):
                    max_iterations = node.data.get("max_iterations")

                if max_iterations is not None:
                    condition_nodes_with_iterations.add(node.id)
                    logger.info(f"Condition ë…¸ë“œ ë°œê²¬ (max_iterations={max_iterations}): {node.id}")
                else:
                    logger.debug(f"Condition ë…¸ë“œ ë°œê²¬ (max_iterations ì—†ìŒ): {node.id}")

        logger.info(f"í”¼ë“œë°± ë£¨í”„ ì œì–´ ë…¸ë“œ ì´ {len(condition_nodes_with_iterations)}ê°œ: {condition_nodes_with_iterations}")

        # ë°±ì—£ì§€ ì‹ë³„ (DFSë¡œ ìˆœí™˜ ê²½ë¡œ íŒŒì•…)
        back_edges = set()
        visited_dfs = set()
        rec_stack = set()

        def identify_back_edges(node_id: str, path: List[str]):
            """DFSë¡œ ë°±ì—£ì§€ ì‹ë³„ (ìˆœí™˜ ê²½ë¡œì— Condition + max_iterations í¬í•¨ ì—¬ë¶€ í™•ì¸)"""
            visited_dfs.add(node_id)
            rec_stack.add(node_id)
            path.append(node_id)

            for edge in valid_edges:
                if edge.source == node_id:
                    target = edge.target

                    # ì´ë¯¸ ë°©ë¬¸ ìŠ¤íƒì— ìˆìœ¼ë©´ ë°±ì—£ì§€ (ìˆœí™˜)
                    if target in rec_stack:
                        logger.debug(f"ğŸ”„ ë°±ì—£ì§€ ë°œê²¬: {node_id} â†’ {target}")

                        # ìˆœí™˜ ê²½ë¡œ ì¶”ì¶œ (targetë¶€í„° í˜„ì¬ ë…¸ë“œê¹Œì§€)
                        try:
                            cycle_start_idx = path.index(target)
                            cycle_path = path[cycle_start_idx:] + [target]
                            logger.debug(f"   ìˆœí™˜ ê²½ë¡œ: {' â†’ '.join(cycle_path)}")
                        except ValueError:
                            logger.error(f"   âŒ ìˆœí™˜ ê²½ë¡œ ì¶”ì¶œ ì‹¤íŒ¨: target={target}, path={path}")
                            continue

                        # ìˆœí™˜ ê²½ë¡œì— max_iterationsê°€ ì„¤ì •ëœ Condition ë…¸ë“œê°€ ìˆëŠ”ì§€ í™•ì¸
                        has_condition_with_iterations = any(
                            node_id in condition_nodes_with_iterations
                            for node_id in cycle_path
                        )
                        logger.debug(f"   Condition ë…¸ë“œ í¬í•¨ ì—¬ë¶€: {has_condition_with_iterations}")

                        if has_condition_with_iterations:
                            # í”¼ë“œë°± ë£¨í”„ í—ˆìš©: ë°±ì—£ì§€ë¡œ í‘œì‹œ
                            back_edges.add((edge.source, edge.target))
                            logger.info(
                                f"âœ… í”¼ë“œë°± ë£¨í”„ ê°ì§€ (í—ˆìš©): {edge.source} â†’ {edge.target} "
                                f"(ìˆœí™˜ ê²½ë¡œ: {' â†’ '.join(cycle_path)})"
                            )
                        else:
                            # max_iterations ì—†ëŠ” ìˆœí™˜: ê²€ì¦ ë‹¨ê³„ì—ì„œ ì—ëŸ¬ ë°œìƒ
                            logger.warning(
                                f"âš ï¸ ë¬´ì œí•œ ìˆœí™˜ ê°ì§€: {edge.source} â†’ {edge.target} "
                                f"(ìˆœí™˜ ê²½ë¡œ: {' â†’ '.join(cycle_path)}). "
                                f"Condition ë…¸ë“œì— max_iterationsë¥¼ ì„¤ì •í•˜ì„¸ìš”."
                            )
                    elif target not in visited_dfs:
                        identify_back_edges(target, path.copy())

            rec_stack.remove(node_id)

        # ëª¨ë“  Input ë…¸ë“œì—ì„œ DFS ì‹œì‘í•˜ì—¬ ë°±ì—£ì§€ ì‹ë³„
        for input_id in input_node_ids:
            if input_id not in visited_dfs:
                identify_back_edges(input_id, [])

        logger.info(f"ë°±ì—£ì§€ ì‹ë³„ ì™„ë£Œ: ì´ {len(back_edges)}ê°œ ë°œê²¬")
        if back_edges:
            for source, target in back_edges:
                logger.info(f"  - {source} â†’ {target}")
        else:
            logger.info("  (ë°±ì—£ì§€ ì—†ìŒ)")

        # ì¸ì ‘ ë¦¬ìŠ¤íŠ¸ (ë…¸ë“œ ID â†’ ìì‹ ë…¸ë“œ ID ëª©ë¡)
        adjacency: Dict[str, List[str]] = {node.id: [] for node in nodes}
        in_degree: Dict[str, int] = {node.id: 0 for node in nodes}

        for edge in valid_edges:
            # ë°±ì—£ì§€ëŠ” ìœ„ìƒ ì •ë ¬ì—ì„œ ì œì™¸ (í”¼ë“œë°± ë£¨í”„)
            if (edge.source, edge.target) not in back_edges:
                adjacency[edge.source].append(edge.target)
                in_degree[edge.target] += 1
            else:
                logger.debug(f"ë°±ì—£ì§€ ì œì™¸ (ìœ„ìƒ ì •ë ¬): {edge.source} â†’ {edge.target}")

        # ë””ë²„ê¹…: ì¸ì ‘ ë¦¬ìŠ¤íŠ¸ ì¶œë ¥
        logger.debug("ìœ„ìƒ ì •ë ¬ìš© ì¸ì ‘ ë¦¬ìŠ¤íŠ¸:")
        for node_id, children in adjacency.items():
            if children:
                logger.debug(f"  {node_id} â†’ {children}")

        # Input ë…¸ë“œì—ì„œ ë„ë‹¬ ê°€ëŠ¥í•œ ë…¸ë“œë§Œ í•„í„°ë§ (BFS)
        reachable_nodes = set(input_node_ids)
        bfs_queue = deque(input_node_ids)

        while bfs_queue:
            current_id = bfs_queue.popleft()
            for child_id in adjacency[current_id]:
                if child_id not in reachable_nodes:
                    reachable_nodes.add(child_id)
                    bfs_queue.append(child_id)

        # ë„ë‹¬ ë¶ˆê°€ëŠ¥í•œ ë…¸ë“œ ê²½ê³ 
        unreachable_nodes = [node.id for node in nodes if node.id not in reachable_nodes]
        if unreachable_nodes:
            logger.warning(
                f"Input ë…¸ë“œì—ì„œ ë„ë‹¬í•  ìˆ˜ ì—†ëŠ” ë…¸ë“œê°€ ìˆìŠµë‹ˆë‹¤: {unreachable_nodes}. "
                "ì´ ë…¸ë“œë“¤ì€ ì‹¤í–‰ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤."
            )

        # ë„ë‹¬ ê°€ëŠ¥í•œ ë…¸ë“œë§Œìœ¼ë¡œ ìœ„ìƒ ì •ë ¬ ìˆ˜í–‰
        # Input ë…¸ë“œë§Œ ì‹œì‘ì ìœ¼ë¡œ ì„¤ì •
        queue = deque(input_node_ids)
        sorted_nodes = []
        visited = set()

        # ë¬´í•œ ë£¨í”„ ë°©ì§€: ìµœëŒ€ ë°˜ë³µ íšŸìˆ˜ (ë…¸ë“œ ìˆ˜ * ë…¸ë“œ ìˆ˜)
        max_iterations = len(reachable_nodes) * len(reachable_nodes)
        iteration_count = 0
        stuck_counter: Dict[str, int] = {}  # ê° ë…¸ë“œê°€ íì— ì¶”ê°€ëœ íšŸìˆ˜

        while queue:
            iteration_count += 1
            if iteration_count > max_iterations:
                stuck_nodes = [nid for nid, count in stuck_counter.items() if count > 5]
                raise ValueError(
                    f"ìœ„ìƒ ì •ë ¬ ì¤‘ ë¬´í•œ ë£¨í”„ ê°ì§€. êµì°© ìƒíƒœ ë…¸ë“œ: {stuck_nodes}. "
                    f"Condition ë…¸ë“œ + max_iterationsë¥¼ í†µí•œ í”¼ë“œë°± ë£¨í”„ê°€ ì•„ë‹Œ ì‹¤ì œ ìˆœí™˜ ì°¸ì¡°ê°€ ìˆì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤."
                )

            node_id = queue.popleft()

            if node_id in visited:
                continue

            # ë„ë‹¬ ë¶ˆê°€ëŠ¥í•œ ë…¸ë“œëŠ” ê±´ë„ˆëœ€
            if node_id not in reachable_nodes:
                continue

            # ëª¨ë“  ë¶€ëª¨ ë…¸ë“œê°€ ì²˜ë¦¬ë˜ì—ˆëŠ”ì§€ í™•ì¸ (ë°±ì—£ì§€ ì œì™¸)
            parents_ready = True
            for edge in valid_edges:
                # ë°±ì—£ì§€ëŠ” ë¶€ëª¨ ì˜ì¡´ì„± ì²´í¬ì—ì„œ ì œì™¸ (í”¼ë“œë°± ë£¨í”„)
                if (edge.source, edge.target) in back_edges:
                    continue

                if edge.target == node_id and edge.source in reachable_nodes:
                    if edge.source not in visited:
                        parents_ready = False
                        break

            if not parents_ready:
                # ë¶€ëª¨ ë…¸ë“œê°€ ì•„ì§ ì²˜ë¦¬ë˜ì§€ ì•Šì•˜ìœ¼ë©´ í ë’¤ë¡œ
                stuck_counter[node_id] = stuck_counter.get(node_id, 0) + 1
                queue.append(node_id)
                continue

            visited.add(node_id)
            sorted_nodes.append(node_map[node_id])

            # ìì‹ ë…¸ë“œë¥¼ íì— ì¶”ê°€
            for child_id in adjacency[node_id]:
                if child_id not in visited and child_id in reachable_nodes:
                    queue.append(child_id)

        # ìˆœí™˜ ì°¸ì¡° ê²€ì‚¬ (ë„ë‹¬ ê°€ëŠ¥í•œ ë…¸ë“œ ê¸°ì¤€)
        if len(sorted_nodes) != len(reachable_nodes):
            unvisited = [nid for nid in reachable_nodes if nid not in visited]
            raise ValueError(
                f"ì›Œí¬í”Œë¡œìš°ì— ìˆœí™˜ ì°¸ì¡°ê°€ ìˆìŠµë‹ˆë‹¤. ë°©ë¬¸í•˜ì§€ ëª»í•œ ë…¸ë“œ: {unvisited}"
            )

        return sorted_nodes

    def _get_parent_nodes(
        self, node_id: str, edges: List[WorkflowEdge]
    ) -> List[str]:
        """
        ë…¸ë“œì˜ ë¶€ëª¨ ë…¸ë“œ ID ëª©ë¡ ì¡°íšŒ

        Args:
            node_id: ë…¸ë“œ ID
            edges: ì—£ì§€ ëª©ë¡

        Returns:
            List[str]: ë¶€ëª¨ ë…¸ë“œ ID ëª©ë¡
        """
        return [edge.source for edge in edges if edge.target == node_id]

    def _get_child_nodes(
        self, node_id: str, edges: List[WorkflowEdge]
    ) -> List[str]:
        """
        ë…¸ë“œì˜ ìì‹ ë…¸ë“œ ID ëª©ë¡ ì¡°íšŒ

        Args:
            node_id: ë…¸ë“œ ID
            edges: ì—£ì§€ ëª©ë¡

        Returns:
            List[str]: ìì‹ ë…¸ë“œ ID ëª©ë¡
        """
        return [edge.target for edge in edges if edge.source == node_id]

    def _check_parallel_execution(self, node: WorkflowNode) -> bool:
        """
        ë…¸ë“œì˜ parallel_execution í”Œë˜ê·¸ í™•ì¸

        Args:
            node: ì›Œí¬í”Œë¡œìš° ë…¸ë“œ

        Returns:
            bool: ë³‘ë ¬ ì‹¤í–‰ ì—¬ë¶€
        """
        if isinstance(node.data, dict):
            return node.data.get("parallel_execution", False)
        else:
            return getattr(node.data, "parallel_execution", False)

    def _compute_execution_groups(
        self, nodes: List[WorkflowNode], edges: List[WorkflowEdge]
    ) -> List[List[WorkflowNode]]:
        """
        ë³‘ë ¬ ì‹¤í–‰ ê·¸ë£¹ ê³„ì‚°

        parallel_execution=Trueì¸ ë…¸ë“œì˜ ìì‹ ë…¸ë“œë“¤ì„ ë³‘ë ¬ ê·¸ë£¹ìœ¼ë¡œ ë¬¶ìŠµë‹ˆë‹¤.

        Args:
            nodes: ìœ„ìƒ ì •ë ¬ëœ ë…¸ë“œ ëª©ë¡
            edges: ì—£ì§€ ëª©ë¡

        Returns:
            List[List[WorkflowNode]]: ì‹¤í–‰ ê·¸ë£¹ ëª©ë¡ (ê° ê·¸ë£¹ì€ ë³‘ë ¬ ì‹¤í–‰)
        """
        node_map = {node.id: node for node in nodes}
        processed = set()
        execution_groups = []

        for node in nodes:
            if node.id in processed:
                continue

            # parallel_execution í”Œë˜ê·¸ í™•ì¸
            if self._check_parallel_execution(node):
                # ìì‹ ë…¸ë“œë“¤ì„ ë³‘ë ¬ ê·¸ë£¹ìœ¼ë¡œ ë¬¶ìŒ
                child_ids = self._get_child_nodes(node.id, edges)
                parallel_group = []

                for child_id in child_ids:
                    if child_id in node_map and child_id not in processed:
                        parallel_group.append(node_map[child_id])
                        processed.add(child_id)

                # í˜„ì¬ ë…¸ë“œëŠ” ë‹¨ë… ì‹¤í–‰
                execution_groups.append([node])
                processed.add(node.id)

                # ìì‹ ë…¸ë“œë“¤ì€ ë³‘ë ¬ ì‹¤í–‰
                if parallel_group:
                    execution_groups.append(parallel_group)
            else:
                # ë‹¨ë… ì‹¤í–‰
                execution_groups.append([node])
                processed.add(node.id)

        return execution_groups

    def _render_task_template(
        self,
        template: str,
        node_id: str,
        node_outputs: Dict[str, str],
        initial_input: str,
    ) -> str:
        """
        ì‘ì—… ì„¤ëª… í…œí”Œë¦¿ ë Œë”ë§

        ë³€ìˆ˜:
        - {{input}}: ì´ˆê¸° ì…ë ¥ (ì²« ë²ˆì§¸ ë…¸ë“œ)
        - {{node_<id>}}: íŠ¹ì • ë…¸ë“œì˜ ì¶œë ¥
        - {{parent}}: ë¶€ëª¨ ë…¸ë“œì˜ ì¶œë ¥ (ë¶€ëª¨ê°€ 1ê°œì¸ ê²½ìš°)

        Args:
            template: í…œí”Œë¦¿ ë¬¸ìì—´
            node_id: í˜„ì¬ ë…¸ë“œ ID
            node_outputs: ë…¸ë“œ ID â†’ ì¶œë ¥ ë§¤í•‘
            initial_input: ì´ˆê¸° ì…ë ¥

        Returns:
            str: ë Œë”ë§ëœ ì‘ì—… ì„¤ëª…
        """
        result = template

        # {{input}} ì¹˜í™˜
        result = result.replace("{{input}}", initial_input)

        # {{node_<id>}} ì¹˜í™˜
        for nid, output in node_outputs.items():
            result = result.replace(f"{{{{node_{nid}}}}}", output)

        # {{parent}} ì¹˜í™˜ (ë¶€ëª¨ê°€ 1ê°œì¸ ê²½ìš°ë§Œ ì§€ì›)
        if "{{parent}}" in result:
            parent_node_ids = list(node_outputs.keys())
            if len(parent_node_ids) == 1:
                # ë¶€ëª¨ê°€ 1ê°œì¸ ê²½ìš°, í•´ë‹¹ ë…¸ë“œì˜ ì¶œë ¥ ì‚¬ìš©
                result = result.replace("{{parent}}", node_outputs[parent_node_ids[0]])
            elif len(parent_node_ids) == 0:
                # ë¶€ëª¨ê°€ ì—†ìœ¼ë©´ ë¹ˆ ë¬¸ìì—´ë¡œ ì¹˜í™˜
                result = result.replace("{{parent}}", "")
            else:
                # ë¶€ëª¨ê°€ ì—¬ëŸ¬ ê°œì¸ ê²½ìš°, ê²½ê³  ë¡œê·¸ ë° ì²« ë²ˆì§¸ ë¶€ëª¨ ì¶œë ¥ ì‚¬ìš©
                logger.warning(
                    f"ë…¸ë“œì— ë¶€ëª¨ê°€ {len(parent_node_ids)}ê°œ ìˆìŠµë‹ˆë‹¤. "
                    f"{{{{parent}}}} ë³€ìˆ˜ëŠ” ë¶€ëª¨ê°€ 1ê°œì¸ ê²½ìš°ë§Œ ì§€ì›í•©ë‹ˆë‹¤. "
                    f"ì²« ë²ˆì§¸ ë¶€ëª¨ì˜ ì¶œë ¥ì„ ì‚¬ìš©í•©ë‹ˆë‹¤: {parent_node_ids[0]}"
                )
                result = result.replace("{{parent}}", node_outputs[parent_node_ids[0]])

        return result

    async def _evaluate_llm_condition(
        self,
        condition_prompt: str,
        input_text: str,
        session_id: str,
    ) -> Tuple[bool, str]:
        """
        LLMì„ ì‚¬ìš©í•˜ì—¬ ì¡°ê±´ í‰ê°€ (Haiku ëª¨ë¸ ì‚¬ìš©)

        Args:
            condition_prompt: LLMì—ê²Œ ì „ë‹¬í•  ì¡°ê±´ í”„ë¡¬í”„íŠ¸
            input_text: í‰ê°€í•  í…ìŠ¤íŠ¸
            session_id: ì„¸ì…˜ ID

        Returns:
            Tuple[bool, str]: (ì¡°ê±´ ê²°ê³¼, LLM ì‘ë‹µ ì´ìœ )
        """
        from claude_agent_sdk import query
        from claude_agent_sdk.types import ClaudeAgentOptions

        logger.info(f"[{session_id}] LLM ì¡°ê±´ í‰ê°€ ì‹œì‘ (Haiku ëª¨ë¸)")

        # Haiku ëª¨ë¸ë¡œ ë¹ ë¥¸ íŒë‹¨
        options = ClaudeAgentOptions(
            model="claude-haiku-4-20250514",
            allowed_tools=[],  # ë„êµ¬ ì‚¬ìš© ì•ˆí•¨
            permission_mode="default",
        )

        # LLMì—ê²Œ ì „ë‹¬í•  ì „ì²´ í”„ë¡¬í”„íŠ¸
        full_prompt = f"""ë‹¤ìŒ ì¶œë ¥ì„ ë¶„ì„í•˜ì—¬ ì¡°ê±´ì„ í‰ê°€í•´ì£¼ì„¸ìš”.

<ì¡°ê±´>
{condition_prompt}
</ì¡°ê±´>

<í‰ê°€ ëŒ€ìƒ ì¶œë ¥>
{input_text[:5000]}  # ì²˜ìŒ 5000ìë§Œ
</í‰ê°€ ëŒ€ìƒ ì¶œë ¥>

ìœ„ ì¶œë ¥ì´ ì¡°ê±´ì„ ë§Œì¡±í•˜ëŠ”ì§€ íŒë‹¨í•˜ì—¬, ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ ì‘ë‹µí•´ì£¼ì„¸ìš”:

íŒë‹¨: [YES ë˜ëŠ” NO]
ì´ìœ : [í•œ ì¤„ ì„¤ëª…]

ì˜ˆì‹œ:
íŒë‹¨: YES
ì´ìœ : í…ŒìŠ¤íŠ¸ê°€ ëª¨ë‘ í†µê³¼í–ˆìœ¼ë©° ì—ëŸ¬ê°€ ì—†ìŠµë‹ˆë‹¤.
"""

        try:
            # LLM í˜¸ì¶œ
            response_text = ""
            async for response in query(prompt=full_prompt, options=options):
                if hasattr(response, 'content') and isinstance(response.content, list):
                    for block in response.content:
                        if hasattr(block, 'type') and block.type == 'text':
                            response_text += block.text

            logger.debug(f"[{session_id}] LLM ì‘ë‹µ: {response_text[:200]}")

            # ì‘ë‹µ íŒŒì‹±
            lines = response_text.strip().split('\n')
            result = False
            reason = ""

            for line in lines:
                if line.startswith('íŒë‹¨:'):
                    decision = line.replace('íŒë‹¨:', '').strip().upper()
                    result = decision in ['YES', 'Y', 'TRUE', 'ì˜ˆ']
                elif line.startswith('ì´ìœ :'):
                    reason = line.replace('ì´ìœ :', '').strip()

            if not reason:
                reason = response_text[:200]  # íŒŒì‹± ì‹¤íŒ¨ ì‹œ ì „ì²´ ì‘ë‹µ ì‚¬ìš©

            logger.info(
                f"[{session_id}] LLM ì¡°ê±´ í‰ê°€ ì™„ë£Œ: {result} (ì´ìœ : {reason[:100]})"
            )

            return result, reason

        except Exception as e:
            logger.error(f"[{session_id}] LLM ì¡°ê±´ í‰ê°€ ì‹¤íŒ¨: {e}", exc_info=True)
            # ì—ëŸ¬ ë°œìƒ ì‹œ ì•ˆì „í•˜ê²Œ False ë°˜í™˜
            return False, f"LLM í‰ê°€ ì‹¤íŒ¨: {str(e)}"

    def _evaluate_condition(
        self,
        condition_type: str,
        condition_value: str,
        input_text: str
    ) -> bool:
        """
        ì¡°ê±´ í‰ê°€

        Args:
            condition_type: ì¡°ê±´ íƒ€ì… ('contains', 'regex', 'length', 'custom', 'llm')
            condition_value: ì¡°ê±´ ê°’
            input_text: í‰ê°€í•  í…ìŠ¤íŠ¸

        Returns:
            bool: ì¡°ê±´ì´ Trueì¸ì§€ ì—¬ë¶€
        """
        import re

        if condition_type == "contains":
            # í…ìŠ¤íŠ¸ í¬í•¨ ê²€ì‚¬
            return condition_value in input_text

        elif condition_type == "regex":
            # ì •ê·œí‘œí˜„ì‹ ë§¤ì¹­
            try:
                pattern = re.compile(condition_value)
                return bool(pattern.search(input_text))
            except re.error as e:
                logger.error(f"ì •ê·œí‘œí˜„ì‹ ì˜¤ë¥˜: {e}")
                return False

        elif condition_type == "length":
            # ê¸¸ì´ ë¹„êµ (ì˜ˆ: ">100", "<=500", "==0")
            try:
                text_length = len(input_text)
                # condition_valueë¥¼ íŒŒì‹±í•˜ì—¬ ë¹„êµ
                if condition_value.startswith(">="):
                    threshold = int(condition_value[2:].strip())
                    return text_length >= threshold
                elif condition_value.startswith("<="):
                    threshold = int(condition_value[2:].strip())
                    return text_length <= threshold
                elif condition_value.startswith(">"):
                    threshold = int(condition_value[1:].strip())
                    return text_length > threshold
                elif condition_value.startswith("<"):
                    threshold = int(condition_value[1:].strip())
                    return text_length < threshold
                elif condition_value.startswith("=="):
                    threshold = int(condition_value[2:].strip())
                    return text_length == threshold
                else:
                    # ìˆ«ìë§Œ ìˆëŠ” ê²½ìš° == ë¡œ ê°„ì£¼
                    threshold = int(condition_value.strip())
                    return text_length == threshold
            except (ValueError, IndexError) as e:
                logger.error(f"ê¸¸ì´ ì¡°ê±´ íŒŒì‹± ì˜¤ë¥˜: {e}")
                return False

        elif condition_type == "custom":
            # ì»¤ìŠ¤í…€ Python í‘œí˜„ì‹ í‰ê°€
            try:
                # ì•ˆì „í•œ í‰ê°€ë¥¼ ìœ„í•´ ì œí•œëœ ë„¤ì„ìŠ¤í˜ì´ìŠ¤ ì‚¬ìš©
                namespace = {
                    "output": input_text,
                    "len": len,
                    "str": str,
                    "int": int,
                    "float": float,
                }
                result = eval(condition_value, {"__builtins__": {}}, namespace)
                return bool(result)
            except Exception as e:
                logger.error(f"ì»¤ìŠ¤í…€ ì¡°ê±´ í‰ê°€ ì˜¤ë¥˜: {e}")
                return False

        else:
            logger.warning(f"ì•Œ ìˆ˜ ì—†ëŠ” ì¡°ê±´ íƒ€ì…: {condition_type}")
            return False

    async def _execute_condition_node(
        self,
        node: WorkflowNode,
        node_outputs: Dict[str, str],
        edges: List[WorkflowEdge],
        session_id: str,
    ) -> tuple[str, str]:
        """
        ì¡°ê±´ ë¶„ê¸° ë…¸ë“œ ì‹¤í–‰ (ë°˜ë³µ ì œí•œ í¬í•¨)

        Args:
            node: ì¡°ê±´ ë…¸ë“œ
            node_outputs: ì´ì „ ë…¸ë“œ ì¶œë ¥ë“¤
            edges: ì—£ì§€ ëª©ë¡ (ë¶„ê¸° ê²½ë¡œ í™•ì¸ìš©)
            session_id: ì„¸ì…˜ ID

        Returns:
            tuple[str, str]: (ë‹¤ìŒ ì‹¤í–‰í•  ë…¸ë“œ ID, ì¡°ê±´ í‰ê°€ ê²°ê³¼ í…ìŠ¤íŠ¸)

        Raises:
            ValueError: ë¶€ëª¨ ë…¸ë“œê°€ ì—†ê±°ë‚˜ ë¶„ê¸° ê²½ë¡œê°€ ì—†ëŠ” ê²½ìš°
        """
        node_id = node.id
        node_data: ConditionNodeData = node.data  # type: ignore

        # ë°˜ë³µ íšŸìˆ˜ ì¦ê°€
        if session_id not in self._condition_iterations:
            self._condition_iterations[session_id] = {}

        current_iteration = self._condition_iterations[session_id].get(node_id, 0) + 1
        self._condition_iterations[session_id][node_id] = current_iteration

        logger.info(
            f"[{session_id}] ì¡°ê±´ ë…¸ë“œ ì‹¤í–‰: {node_id} "
            f"(íƒ€ì…: {node_data.condition_type}, ë°˜ë³µ: {current_iteration}íšŒ)"
        )

        # ë¶€ëª¨ ë…¸ë“œ ì¶œë ¥ ê°€ì ¸ì˜¤ê¸°
        parent_nodes = self._get_parent_nodes(node_id, edges)
        if not parent_nodes:
            raise ValueError(f"ì¡°ê±´ ë…¸ë“œ {node_id}ì— ë¶€ëª¨ ë…¸ë“œê°€ ì—†ìŠµë‹ˆë‹¤")

        # ì²« ë²ˆì§¸ ë¶€ëª¨ ë…¸ë“œì˜ ì¶œë ¥ ì‚¬ìš©
        parent_id = parent_nodes[0]
        parent_output = node_outputs.get(parent_id, "")

        # LLM ì¡°ê±´ì¸ ê²½ìš° ë¹„ë™ê¸° í‰ê°€
        llm_reason = ""
        if node_data.condition_type == "llm":
            condition_result, llm_reason = await self._evaluate_llm_condition(
                node_data.condition_value,
                parent_output,
                session_id
            )
        else:
            # ì¼ë°˜ ì¡°ê±´ í‰ê°€
            condition_result = self._evaluate_condition(
                node_data.condition_type,
                node_data.condition_value,
                parent_output
            )

        logger.info(
            f"[{session_id}] ì¡°ê±´ í‰ê°€ ê²°ê³¼: {condition_result} "
            f"(ì…ë ¥ ê¸¸ì´: {len(parent_output)})"
        )

        # max_iterations ì²´í¬ (ë°˜ë³µ ì œí•œ)
        # max_iterationsê°€ Noneì¸ ê²½ìš° ê¸°ë³¸ê°’ 10 ì‚¬ìš©
        max_iterations = node_data.max_iterations if node_data.max_iterations is not None else 10

        if current_iteration >= max_iterations:
            logger.warning(
                f"[{session_id}] ì¡°ê±´ ë…¸ë“œ {node_id}: "
                f"ìµœëŒ€ ë°˜ë³µ íšŸìˆ˜ ë„ë‹¬ ({current_iteration}/{max_iterations}). "
                f"ê°•ì œë¡œ true ê²½ë¡œë¡œ ì´ë™í•©ë‹ˆë‹¤."
            )
            # ìµœëŒ€ ë°˜ë³µ íšŸìˆ˜ ë„ë‹¬ ì‹œ ê°•ì œë¡œ true ê²½ë¡œë¡œ ì´ë™
            condition_result = True
            llm_reason = f"ìµœëŒ€ ë°˜ë³µ íšŸìˆ˜ ë„ë‹¬ ({max_iterations}íšŒ)"

        # ë¶„ê¸° ê²½ë¡œ ê²°ì • (ì—£ì§€ì˜ sourceHandleì„ ì‚¬ìš©)
        # sourceHandleì´ "true"ì¸ ì—£ì§€ â†’ True ê²½ë¡œ
        # sourceHandleì´ "false"ì¸ ì—£ì§€ â†’ False ê²½ë¡œ
        next_node_id = None
        for edge in edges:
            if edge.source == node_id:
                if condition_result and edge.sourceHandle == "true":
                    next_node_id = edge.target
                    break
                elif not condition_result and edge.sourceHandle == "false":
                    next_node_id = edge.target
                    break

        if next_node_id is None:
            branch_type = "true" if condition_result else "false"
            raise ValueError(
                f"ì¡°ê±´ ë…¸ë“œ {node_id}ì˜ {branch_type} ë¶„ê¸° ê²½ë¡œê°€ ì—†ìŠµë‹ˆë‹¤. "
                f"sourceHandleì´ '{branch_type}'ì¸ ì—£ì§€ë¥¼ ì¶”ê°€í•´ì£¼ì„¸ìš”."
            )

        # ì¡°ê±´ ê²°ê³¼ë¥¼ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜
        result_text = f"ì¡°ê±´ í‰ê°€ ê²°ê³¼: {condition_result}\n"
        result_text += f"ë°˜ë³µ íšŸìˆ˜: {current_iteration}/{max_iterations}"
        result_text += f"\në¶„ê¸°: {next_node_id}"

        if llm_reason:
            result_text += f"\nLLM íŒë‹¨ ì´ìœ : {llm_reason}"

        return next_node_id, result_text

    async def _execute_merge_node(
        self,
        node: WorkflowNode,
        node_outputs: Dict[str, str],
        edges: List[WorkflowEdge],
        session_id: str,
    ) -> str:
        """
        ë³‘í•© ë…¸ë“œ ì‹¤í–‰

        Args:
            node: ë³‘í•© ë…¸ë“œ
            node_outputs: ì´ì „ ë…¸ë“œ ì¶œë ¥ë“¤
            edges: ì—£ì§€ ëª©ë¡
            session_id: ì„¸ì…˜ ID

        Returns:
            str: ë³‘í•©ëœ ì¶œë ¥
        """
        node_id = node.id
        node_data: MergeNodeData = node.data  # type: ignore

        logger.info(
            f"[{session_id}] ë³‘í•© ë…¸ë“œ ì‹¤í–‰: {node_id} "
            f"(ì „ëµ: {node_data.merge_strategy})"
        )

        # ë¶€ëª¨ ë…¸ë“œ ì¶œë ¥ë“¤ ìˆ˜ì§‘
        parent_nodes = self._get_parent_nodes(node_id, edges)
        if not parent_nodes:
            raise ValueError(f"ë³‘í•© ë…¸ë“œ {node_id}ì— ë¶€ëª¨ ë…¸ë“œê°€ ì—†ìŠµë‹ˆë‹¤")

        parent_outputs = []
        for pid in parent_nodes:
            if pid not in node_outputs:
                logger.warning(
                    f"[{session_id}] ë³‘í•© ë…¸ë“œ {node_id}: "
                    f"ë¶€ëª¨ ë…¸ë“œ '{pid}'ì˜ ì¶œë ¥ì´ ì—†ìŠµë‹ˆë‹¤. ë¹ˆ ë¬¸ìì—´ì„ ì‚¬ìš©í•©ë‹ˆë‹¤."
                )
            parent_outputs.append(node_outputs.get(pid, ""))

        # ë³‘í•© ì „ëµì— ë”°ë¼ ì¶œë ¥ ìƒì„±
        if node_data.merge_strategy == "concatenate":
            # ëª¨ë“  ì¶œë ¥ì„ êµ¬ë¶„ìë¡œ ê²°í•©
            merged_output = node_data.separator.join(parent_outputs)

        elif node_data.merge_strategy == "first":
            # ì²« ë²ˆì§¸ ì¶œë ¥ë§Œ ì‚¬ìš©
            merged_output = parent_outputs[0] if parent_outputs else ""

        elif node_data.merge_strategy == "last":
            # ë§ˆì§€ë§‰ ì¶œë ¥ë§Œ ì‚¬ìš©
            merged_output = parent_outputs[-1] if parent_outputs else ""

        elif node_data.merge_strategy == "custom":
            # ì»¤ìŠ¤í…€ í…œí”Œë¦¿ ì‚¬ìš©
            if node_data.custom_template:
                merged_output = node_data.custom_template
                for i, output in enumerate(parent_outputs):
                    merged_output = merged_output.replace(f"{{{{branch_{i+1}}}}}", output)
            else:
                # í…œí”Œë¦¿ì´ ì—†ìœ¼ë©´ concatenateë¡œ í´ë°±
                merged_output = node_data.separator.join(parent_outputs)

        else:
            logger.warning(
                f"ì•Œ ìˆ˜ ì—†ëŠ” ë³‘í•© ì „ëµ: {node_data.merge_strategy}, "
                "concatenateë¡œ í´ë°±í•©ë‹ˆë‹¤"
            )
            merged_output = node_data.separator.join(parent_outputs)

        logger.info(
            f"[{session_id}] ë³‘í•© ë…¸ë“œ ì™„ë£Œ: {node_id} "
            f"(ì…ë ¥: {len(parent_outputs)}ê°œ, ì¶œë ¥ ê¸¸ì´: {len(merged_output)})"
        )

        return merged_output

    async def _execute_single_node(
        self,
        node: WorkflowNode,
        node_outputs: Dict[str, str],
        initial_input: str,
        session_id: str,
        edges: List[WorkflowEdge],
        all_nodes: List[WorkflowNode],
        project_path: Optional[str] = None,
    ) -> AsyncIterator[WorkflowNodeExecutionEvent]:
        """
        ë‹¨ì¼ ë…¸ë“œ ì‹¤í–‰ (ëª¨ë“  ë…¸ë“œ íƒ€ì… ì§€ì›)

        Args:
            node: ì‹¤í–‰í•  ë…¸ë“œ
            node_outputs: ì´ì „ ë…¸ë“œ ì¶œë ¥ë“¤
            initial_input: ì´ˆê¸° ì…ë ¥
            session_id: ì„¸ì…˜ ID
            edges: ì—£ì§€ ëª©ë¡
            all_nodes: ëª¨ë“  ë…¸ë“œ ëª©ë¡
            project_path: í”„ë¡œì íŠ¸ ë””ë ‰í† ë¦¬ ê²½ë¡œ

        Yields:
            WorkflowNodeExecutionEvent: ë…¸ë“œ ì‹¤í–‰ ì´ë²¤íŠ¸
        """
        node_id = node.id

        # Input ë…¸ë“œ ì²˜ë¦¬
        if node.type == "input":
            if isinstance(node.data, InputNodeData):
                input_value = node.data.initial_input
            elif isinstance(node.data, dict):
                input_value = node.data.get("initial_input", initial_input)
            else:
                input_value = initial_input
            node_outputs[node_id] = input_value

            yield WorkflowNodeExecutionEvent(
                event_type="node_start",
                node_id=node_id,
                data={
                    "agent_name": "Input",
                    "input": input_value,  # ë…¸ë“œ ì…ë ¥ ì¶”ê°€ (ë””ë²„ê¹…ìš©)
                },
                timestamp=datetime.now().isoformat(),
            )

            yield WorkflowNodeExecutionEvent(
                event_type="node_complete",
                node_id=node_id,
                data={
                    "node_type": "input",
                    "agent_name": "Input",
                    "output_length": len(input_value),
                    "output": input_value,
                },
                timestamp=datetime.now().isoformat(),
                elapsed_time=0.0,
            )

            logger.info(
                f"[{session_id}] Input ë…¸ë“œ ì™„ë£Œ: {node_id} "
                f"(ì¶œë ¥ ê¸¸ì´: {len(input_value)})"
            )
            return

        # Condition ë…¸ë“œ
        elif node.type == "condition":
            start_time = time.time()

            # ë¶€ëª¨ ë…¸ë“œ ì¶œë ¥ ê°€ì ¸ì˜¤ê¸° (ì…ë ¥ìœ¼ë¡œ ì‚¬ìš©)
            parent_nodes = self._get_parent_nodes(node_id, edges)
            parent_output = ""
            if parent_nodes:
                parent_id = parent_nodes[0]
                parent_output = node_outputs.get(parent_id, "")

            node_data: ConditionNodeData = node.data  # type: ignore

            yield WorkflowNodeExecutionEvent(
                event_type="node_start",
                node_id=node_id,
                data={
                    "node_type": "condition",
                    "input": parent_output,
                    "condition_type": node_data.condition_type,
                    "condition_value": node_data.condition_value,
                },
                timestamp=datetime.now().isoformat(),
            )

            try:
                next_node_id, result_text = await self._execute_condition_node(
                    node, node_outputs, edges, session_id
                )

                node_outputs[node_id] = result_text
                elapsed_time = time.time() - start_time

                yield WorkflowNodeExecutionEvent(
                    event_type="node_complete",
                    node_id=node_id,
                    data={
                        "node_type": "condition",
                        "next_node": next_node_id,
                        "output": result_text,
                    },
                    timestamp=datetime.now().isoformat(),
                    elapsed_time=elapsed_time,
                )

                logger.info(
                    f"[{session_id}] ì¡°ê±´ ë…¸ë“œ ì™„ë£Œ: {node_id} â†’ {next_node_id}"
                )

            except Exception as e:
                error_msg = f"ì¡°ê±´ ë…¸ë“œ ì‹¤í–‰ ì‹¤íŒ¨: {str(e)}"
                logger.error(f"[{session_id}] {node_id}: {error_msg}", exc_info=True)

                elapsed_time = time.time() - start_time

                yield WorkflowNodeExecutionEvent(
                    event_type="node_error",
                    node_id=node_id,
                    data={"error": error_msg},
                    timestamp=datetime.now().isoformat(),
                    elapsed_time=elapsed_time,
                )

                raise

        # Merge ë…¸ë“œ
        elif node.type == "merge":
            start_time = time.time()

            # ë¶€ëª¨ ë…¸ë“œ ì¶œë ¥ë“¤ ê°€ì ¸ì˜¤ê¸° (ì…ë ¥ìœ¼ë¡œ ì‚¬ìš©)
            parent_nodes = self._get_parent_nodes(node_id, edges)
            parent_outputs_list = []
            for pid in parent_nodes:
                parent_outputs_list.append(node_outputs.get(pid, ""))

            node_data: MergeNodeData = node.data  # type: ignore

            # ì…ë ¥ ìš”ì•½ (ê° ë¶€ëª¨ ë…¸ë“œì˜ ì¶œë ¥ ê¸¸ì´)
            input_summary = {
                f"parent_{i+1}": len(output) for i, output in enumerate(parent_outputs_list)
            }

            yield WorkflowNodeExecutionEvent(
                event_type="node_start",
                node_id=node_id,
                data={
                    "node_type": "merge",
                    "input": "\n\n---\n\n".join(parent_outputs_list),
                    "input_summary": input_summary,
                    "merge_strategy": node_data.merge_strategy,
                },
                timestamp=datetime.now().isoformat(),
            )

            try:
                merged_output = await self._execute_merge_node(
                    node, node_outputs, edges, session_id
                )

                node_outputs[node_id] = merged_output
                elapsed_time = time.time() - start_time

                yield WorkflowNodeExecutionEvent(
                    event_type="node_complete",
                    node_id=node_id,
                    data={
                        "node_type": "merge",
                        "output_length": len(merged_output),
                        "output": merged_output,
                    },
                    timestamp=datetime.now().isoformat(),
                    elapsed_time=elapsed_time,
                )

                logger.info(
                    f"[{session_id}] ë³‘í•© ë…¸ë“œ ì™„ë£Œ: {node_id} "
                    f"(ì¶œë ¥ ê¸¸ì´: {len(merged_output)})"
                )

            except Exception as e:
                error_msg = f"ë³‘í•© ë…¸ë“œ ì‹¤í–‰ ì‹¤íŒ¨: {str(e)}"
                logger.error(f"[{session_id}] {node_id}: {error_msg}", exc_info=True)

                elapsed_time = time.time() - start_time

                yield WorkflowNodeExecutionEvent(
                    event_type="node_error",
                    node_id=node_id,
                    data={"error": error_msg},
                    timestamp=datetime.now().isoformat(),
                    elapsed_time=elapsed_time,
                )

                raise

        # Worker ë…¸ë“œ
        else:
            if isinstance(node.data, dict):
                agent_name = node.data.get("agent_name")
                task_template = node.data.get("task_template")
                allowed_tools_override = node.data.get("allowed_tools")
                thinking_override = node.data.get("thinking")

                if not agent_name:
                    raise ValueError(f"ë…¸ë“œ {node_id}: agent_nameì´ ì§€ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
                if not task_template:
                    raise ValueError(f"ë…¸ë“œ {node_id}: task_templateì´ ì§€ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
            else:
                node_data: WorkerNodeData = node.data  # type: ignore
                agent_name = node_data.agent_name
                task_template = node_data.task_template
                allowed_tools_override = node_data.allowed_tools
                thinking_override = node_data.thinking

            start_time = time.time()

            # ë¨¼ì € task_description ìƒì„± (ì…ë ¥ ì €ì¥ìš©)
            agent_config = self._get_agent_config(agent_name)

            if allowed_tools_override is not None:
                agent_config = replace(agent_config, allowed_tools=allowed_tools_override)
                logger.info(
                    f"[{session_id}] ë…¸ë“œ {node_id}: allowed_tools ì˜¤ë²„ë¼ì´ë“œ "
                    f"({len(allowed_tools_override)}ê°œ ë„êµ¬)"
                )

            if thinking_override is not None:
                agent_config = replace(agent_config, thinking=thinking_override)
                logger.info(
                    f"[{session_id}] ë…¸ë“œ {node_id}: thinking ëª¨ë“œ ì˜¤ë²„ë¼ì´ë“œ "
                    f"(thinking={thinking_override})"
                )

            parent_nodes = self._get_parent_nodes(node_id, edges)
            parent_outputs = {
                pid: node_outputs[pid] for pid in parent_nodes
                if pid in node_outputs
            }

            task_description = self._render_task_template(
                template=task_template,
                node_id=node_id,
                node_outputs=parent_outputs,
                initial_input=initial_input,
            )

            # node_start ì´ë²¤íŠ¸
            start_event = WorkflowNodeExecutionEvent(
                event_type="node_start",
                node_id=node_id,
                data={
                    "agent_name": agent_name,
                },
                timestamp=datetime.now().isoformat(),
            )
            logger.info(f"[{session_id}] ğŸŸ¢ ì´ë²¤íŠ¸ ìƒì„±: node_start (node: {node_id}, agent: {agent_name})")
            yield start_event

            # ì…ë ¥ ì´ë²¤íŠ¸ (ë³„ë„ ì´ë²¤íŠ¸ë¡œ ì „ì†¡)
            input_event = WorkflowNodeExecutionEvent(
                event_type="node_output",
                node_id=node_id,
                data={
                    "chunk": task_description,
                    "chunk_type": "input",  # ì…ë ¥ì„ì„ ëª…ì‹œ
                },
            )
            logger.debug(f"[{session_id}] ğŸ“¥ ì´ë²¤íŠ¸ ìƒì„±: node_input (node: {node_id})")
            yield input_event

            try:

                logger.info(
                    f"[{session_id}] ë…¸ë“œ ì‹¤í–‰: {node_id} ({agent_name}) "
                    f"- ì‘ì—… ê¸¸ì´: {len(task_description)}"
                )

                # ë…¸ë“œë³„ ì„¸ì…˜ ê´€ë¦¬: ì´ì „ ì„¸ì…˜ IDê°€ ìˆìœ¼ë©´ ì¬ì‚¬ìš©
                previous_session_id = self._node_sessions.get(node_id)
                if previous_session_id:
                    logger.info(
                        f"[{session_id}] ë…¸ë“œ {node_id}: ì´ì „ ì„¸ì…˜ ì¬ê°œ "
                        f"(ì„¸ì…˜: {previous_session_id[:8]}...)"
                    )
                else:
                    logger.info(
                        f"[{session_id}] ë…¸ë“œ {node_id}: ìƒˆ ì„¸ì…˜ ì‹œì‘"
                    )

                worker = WorkerAgent(config=agent_config, project_dir=project_path)
                node_output_chunks = []
                node_token_usage: Optional[TokenUsage] = None

                def usage_callback(usage_info: Dict[str, Any]):
                    nonlocal node_token_usage
                    node_token_usage = TokenUsage(
                        input_tokens=usage_info.get("input_tokens", 0),
                        output_tokens=usage_info.get("output_tokens", 0),
                        total_tokens=usage_info.get("total_tokens", 0),
                    )
                    logger.debug(
                        f"[{session_id}] ğŸ’° í† í° ì‚¬ìš©ëŸ‰: {node_token_usage.total_tokens} "
                        f"(ì…ë ¥: {node_token_usage.input_tokens}, ì¶œë ¥: {node_token_usage.output_tokens})"
                    )

                # Worker ì‹¤í–‰ (ì´ì „ ì„¸ì…˜ ID ì „ë‹¬ - resume ìš©ë„)
                async for chunk in worker.execute_task(
                    task_description,
                    usage_callback=usage_callback,
                    resume_session_id=previous_session_id
                ):
                    node_output_chunks.append(chunk)

                    # ì²­í¬ íƒ€ì… ë¶„ë¥˜
                    chunk_type = classify_chunk_type(chunk)

                    output_event = WorkflowNodeExecutionEvent(
                        event_type="node_output",
                        node_id=node_id,
                        data={
                            "chunk": chunk,
                            "chunk_type": chunk_type,  # "thinking", "tool", "text"
                        },
                    )
                    logger.debug(f"[{session_id}] ğŸ“ ì´ë²¤íŠ¸ ìƒì„±: node_output (node: {node_id}, type: {chunk_type}, chunk: {len(chunk)}ì)")
                    yield output_event

                # ì „ì²´ ì¶œë ¥ì—ì„œ ìµœì¢… í…ìŠ¤íŠ¸ë§Œ ì¶”ì¶œí•˜ì—¬ ì €ì¥
                full_output = "".join(node_output_chunks)
                final_text = extract_text_from_worker_output(full_output)
                node_outputs[node_id] = final_text  # ë‹¤ìŒ ë…¸ë“œì—ëŠ” ìµœì¢… í…ìŠ¤íŠ¸ë§Œ ì „ë‹¬

                logger.info(
                    f"[{session_id}] ë…¸ë“œ ì¶œë ¥ ì²˜ë¦¬ ì™„ë£Œ: {node_id} "
                    f"(ì „ì²´: {len(full_output)}ì, ìµœì¢… í…ìŠ¤íŠ¸: {len(final_text)}ì)"
                )

                # Workerì—ì„œ ë°˜í™˜ëœ ì‹¤ì œ SDK ì„¸ì…˜ ID ì €ì¥
                if worker.last_session_id:
                    self._node_sessions[node_id] = worker.last_session_id
                    logger.info(
                        f"[{session_id}] ë…¸ë“œ ì„¸ì…˜ ì €ì¥: {node_id} â†’ "
                        f"SDK ì„¸ì…˜ {worker.last_session_id[:8]}... "
                        "(ë‹¤ìŒ ì‹¤í–‰ì—ì„œ ì»¨í…ìŠ¤íŠ¸ ì¬í™œìš©)"
                    )
                else:
                    logger.warning(
                        f"[{session_id}] ë…¸ë“œ {node_id}: SDK ì„¸ì…˜ IDë¥¼ ë°›ì§€ ëª»í•¨"
                    )

                elapsed_time = time.time() - start_time

                complete_event = WorkflowNodeExecutionEvent(
                    event_type="node_complete",
                    node_id=node_id,
                    data={
                        "agent_name": agent_name,
                        "output_length": len(final_text),
                    },
                    timestamp=datetime.now().isoformat(),
                    elapsed_time=elapsed_time,
                    token_usage=node_token_usage,
                )
                logger.info(f"[{session_id}] âœ… ì´ë²¤íŠ¸ ìƒì„±: node_complete (node: {node_id}, agent: {agent_name})")
                yield complete_event

                logger.info(
                    f"[{session_id}] ë…¸ë“œ ì™„ë£Œ: {node_id} ({agent_name}) "
                    f"- ì¶œë ¥ ê¸¸ì´: {len(final_text)}"
                )

            except Exception as e:
                error_msg = f"ë…¸ë“œ ì‹¤í–‰ ì‹¤íŒ¨: {str(e)}"
                logger.error(f"[{session_id}] {node_id}: {error_msg}", exc_info=True)

                elapsed_time = time.time() - start_time

                error_event = WorkflowNodeExecutionEvent(
                    event_type="node_error",
                    node_id=node_id,
                    data={"error": error_msg},
                    timestamp=datetime.now().isoformat(),
                    elapsed_time=elapsed_time,
                )
                logger.error(f"[{session_id}] ğŸ”´ ì´ë²¤íŠ¸ ìƒì„±: node_error (node: {node_id})")
                yield error_event

                raise

    async def _execute_node_and_queue_events(
        self,
        node: WorkflowNode,
        node_outputs: Dict[str, str],
        initial_input: str,
        session_id: str,
        edges: List[WorkflowEdge],
        all_nodes: List[WorkflowNode],
        event_queue: asyncio.Queue,
        project_path: Optional[str] = None,
    ) -> None:
        """
        ë‹¨ì¼ ë…¸ë“œë¥¼ ì‹¤í–‰í•˜ê³  ëª¨ë“  ì´ë²¤íŠ¸ë¥¼ íì— ì „ì†¡

        Args:
            node: ì‹¤í–‰í•  ë…¸ë“œ
            node_outputs: ë…¸ë“œ ì¶œë ¥ ë”•ì…”ë„ˆë¦¬ (ê³µìœ )
            initial_input: ì´ˆê¸° ì…ë ¥
            session_id: ì„¸ì…˜ ID
            edges: ì—£ì§€ ëª©ë¡
            all_nodes: ëª¨ë“  ë…¸ë“œ ëª©ë¡
            event_queue: ì´ë²¤íŠ¸ë¥¼ ì „ì†¡í•  í
            project_path: í”„ë¡œì íŠ¸ ê²½ë¡œ
        """
        try:
            async for event in self._execute_single_node(
                node, node_outputs, initial_input, session_id,
                edges, all_nodes, project_path
            ):
                await event_queue.put(event)
        except Exception as e:
            # ì—ëŸ¬ ë°œìƒ ì‹œ ì—ëŸ¬ë¥¼ íì— ì „ë‹¬
            logger.error(
                f"[{session_id}] ë…¸ë“œ {node.id} ì‹¤í–‰ ì¤‘ ì—ëŸ¬: {str(e)}",
                exc_info=True
            )
            await event_queue.put(e)  # ì˜ˆì™¸ë¥¼ íì— ë„£ìŒ

    async def execute_workflow(
        self,
        workflow: Workflow,
        initial_input: str,
        session_id: str,
        project_path: Optional[str] = None,
        start_node_id: Optional[str] = None,
    ) -> AsyncIterator[WorkflowNodeExecutionEvent]:
        """
        ì›Œí¬í”Œë¡œìš° ì‹¤í–‰ (ìŠ¤íŠ¸ë¦¬ë°, ë³‘ë ¬ ì‹¤í–‰ ì§€ì›)

        Args:
            workflow: ì‹¤í–‰í•  ì›Œí¬í”Œë¡œìš°
            initial_input: ì´ˆê¸° ì…ë ¥ ë°ì´í„°
            session_id: ì„¸ì…˜ ID
            project_path: í”„ë¡œì íŠ¸ ë””ë ‰í† ë¦¬ ê²½ë¡œ (ì„¸ì…˜ë³„ ë¡œê·¸ ì €ì¥ìš©)
            start_node_id: ì‹œì‘ ë…¸ë“œ ID (ì˜µì…˜, ì§€ì • ì‹œ í•´ë‹¹ Input ë…¸ë“œì—ì„œë§Œ ì‹œì‘)

        Yields:
            WorkflowNodeExecutionEvent: ë…¸ë“œ ì‹¤í–‰ ì´ë²¤íŠ¸

        Raises:
            ValueError: ì›Œí¬í”Œë¡œìš° ì„¤ì • ì˜¤ë¥˜
            Exception: ë…¸ë“œ ì‹¤í–‰ ì‹¤íŒ¨
        """
        # ì„¸ì…˜ë³„ íŒŒì¼ í•¸ë“¤ëŸ¬ ì¶”ê°€
        add_session_file_handlers(session_id, project_path)

        # ì„¸ì…˜ë³„ Condition ë…¸ë“œ ë°˜ë³µ íšŸìˆ˜ ì´ˆê¸°í™”
        self._condition_iterations[session_id] = {}

        # ì‹¤í–‰ ì¤‘ì¸ ë³‘ë ¬ íƒœìŠ¤í¬ ì¶”ì  (ì·¨ì†Œ ì‹œ ì •ë¦¬ìš©)
        running_tasks: List[asyncio.Task] = []

        try:
            logger.info(
                f"[{session_id}] ì›Œí¬í”Œë¡œìš° ì‹¤í–‰ ì‹œì‘: {workflow.name} "
                f"(ë…¸ë“œ: {len(workflow.nodes)}, ì—£ì§€: {len(workflow.edges)})"
            )

            # ìœ„ìƒ ì •ë ¬
            try:
                sorted_nodes = self._topological_sort(workflow.nodes, workflow.edges, start_node_id)
            except ValueError as e:
                logger.error(f"[{session_id}] ì›Œí¬í”Œë¡œìš° ì •ë ¬ ì‹¤íŒ¨: {e}")
                raise

            logger.info(
                f"[{session_id}] ì‹¤í–‰ ìˆœì„œ: "
                f"{[node.id for node in sorted_nodes]}"
            )

            # ì‹¤í–‰ ê·¸ë£¹ ê³„ì‚° (ë³‘ë ¬ ì‹¤í–‰ ê·¸ë£¹ í¬í•¨)
            execution_groups = self._compute_execution_groups(sorted_nodes, workflow.edges)

            logger.info(
                f"[{session_id}] ì‹¤í–‰ ê·¸ë£¹: {len(execution_groups)}ê°œ "
                f"(ë³‘ë ¬ ê·¸ë£¹: {sum(1 for g in execution_groups if len(g) > 1)}ê°œ)"
            )

            # ë…¸ë“œ ì¶œë ¥ ì €ì¥ (ë…¸ë“œ ID â†’ ì¶œë ¥)
            node_outputs: Dict[str, str] = {}

            # ì‹¤í–‰ ê·¸ë£¹ë³„ë¡œ ì²˜ë¦¬ (ë³‘ë ¬ ì‹¤í–‰ ì§€ì›)
            for group_idx, group in enumerate(execution_groups):
                group_node_ids = [node.id for node in group]

                if len(group) == 1:
                    # ë‹¨ë… ì‹¤í–‰
                    node = group[0]
                    logger.info(
                        f"[{session_id}] ê·¸ë£¹ {group_idx + 1}/{len(execution_groups)}: "
                        f"ë…¸ë“œ {node.id} ë‹¨ë… ì‹¤í–‰"
                    )

                    async for event in self._execute_single_node(
                        node, node_outputs, initial_input, session_id,
                        workflow.edges, workflow.nodes, project_path
                    ):
                        yield event

                else:
                    # ë³‘ë ¬ ì‹¤í–‰ (ì‹¤ì‹œê°„ ì´ë²¤íŠ¸ ìŠ¤íŠ¸ë¦¬ë°)
                    logger.info(
                        f"[{session_id}] ê·¸ë£¹ {group_idx + 1}/{len(execution_groups)}: "
                        f"{len(group)}ê°œ ë…¸ë“œ ë³‘ë ¬ ì‹¤í–‰ ({group_node_ids})"
                    )

                    # ì´ë²¤íŠ¸ í ìƒì„±
                    event_queue: asyncio.Queue = asyncio.Queue()

                    # ë³‘ë ¬ ì‹¤í–‰ íƒœìŠ¤í¬ ìƒì„±
                    tasks = [
                        asyncio.create_task(
                            self._execute_node_and_queue_events(
                                node, node_outputs, initial_input, session_id,
                                workflow.edges, workflow.nodes, event_queue, project_path
                            )
                        )
                        for node in group
                    ]

                    # ì‹¤í–‰ ì¤‘ì¸ íƒœìŠ¤í¬ ì¶”ì ì— ì¶”ê°€
                    running_tasks.extend(tasks)

                    # ì™„ë£Œëœ ë…¸ë“œ ìˆ˜ ì¶”ì 
                    completed_nodes = 0
                    total_nodes = len(group)

                    # ì‹¤ì‹œê°„ìœ¼ë¡œ ì´ë²¤íŠ¸ë¥¼ ìŠ¤íŠ¸ë¦¬ë°
                    while completed_nodes < total_nodes:
                        # íì—ì„œ ì´ë²¤íŠ¸ ê°€ì ¸ì˜¤ê¸° (íƒ€ì„ì•„ì›ƒ 1ì´ˆ)
                        try:
                            event_or_exception = await asyncio.wait_for(
                                event_queue.get(), timeout=1.0
                            )

                            # ì˜ˆì™¸ì¸ ê²½ìš°
                            if isinstance(event_or_exception, Exception):
                                error_msg = f"ë³‘ë ¬ ì‹¤í–‰ ì¤‘ ë…¸ë“œ ì‹¤íŒ¨: {str(event_or_exception)}"
                                logger.error(f"[{session_id}] {error_msg}", exc_info=event_or_exception)

                                # ì—ëŸ¬ ì´ë²¤íŠ¸ ìƒì„±
                                yield WorkflowNodeExecutionEvent(
                                    event_type="node_error",
                                    node_id="unknown",
                                    data={"error": error_msg},
                                    timestamp=datetime.now().isoformat(),
                                )

                                # ëª¨ë“  íƒœìŠ¤í¬ ì·¨ì†Œ
                                for task in tasks:
                                    task.cancel()

                                raise event_or_exception

                            # ì •ìƒ ì´ë²¤íŠ¸ì¸ ê²½ìš°
                            event = event_or_exception
                            yield event

                            # ë…¸ë“œ ì™„ë£Œ/ì—ëŸ¬ ì´ë²¤íŠ¸ ì¹´ìš´íŒ…
                            if event.event_type in ["node_complete", "node_error"]:
                                completed_nodes += 1
                                logger.info(
                                    f"[{session_id}] ë³‘ë ¬ ë…¸ë“œ ì™„ë£Œ: {event.node_id} "
                                    f"({completed_nodes}/{total_nodes})"
                                )

                        except asyncio.TimeoutError:
                            # íƒ€ì„ì•„ì›ƒ ì‹œ íƒœìŠ¤í¬ ìƒíƒœ í™•ì¸
                            done_tasks = [t for t in tasks if t.done()]
                            if done_tasks:
                                # ì™„ë£Œëœ íƒœìŠ¤í¬ê°€ ìˆìœ¼ë©´ ë‹¤ì‹œ ì‹œë„
                                continue
                            else:
                                # ëª¨ë“  íƒœìŠ¤í¬ê°€ ì•„ì§ ì‹¤í–‰ ì¤‘
                                continue

                    # ëª¨ë“  íƒœìŠ¤í¬ ì™„ë£Œ ëŒ€ê¸° (ì •ë¦¬ ì‘ì—…)
                    await asyncio.gather(*tasks, return_exceptions=True)

                    logger.info(
                        f"[{session_id}] ë³‘ë ¬ ê·¸ë£¹ ì™„ë£Œ: {group_node_ids}"
                    )

            logger.info(f"[{session_id}] ì›Œí¬í”Œë¡œìš° ì‹¤í–‰ ì™„ë£Œ: {workflow.name}")

            # ì›Œí¬í”Œë¡œìš° ì™„ë£Œ ì´ë²¤íŠ¸
            workflow_complete_event = WorkflowNodeExecutionEvent(
                event_type="workflow_complete",
                node_id="",
                data={"message": "ì›Œí¬í”Œë¡œìš° ì‹¤í–‰ ì™„ë£Œ"},
                timestamp=datetime.now().isoformat(),
            )
            logger.info(f"[{session_id}] ğŸ‰ ì´ë²¤íŠ¸ ìƒì„±: workflow_complete")
            yield workflow_complete_event

        except asyncio.CancelledError:
            # ì›Œí¬í”Œë¡œìš° ì·¨ì†Œ ìš”ì²­ ì‹œ
            logger.warning(
                f"[{session_id}] ì›Œí¬í”Œë¡œìš° ì·¨ì†Œ ìš”ì²­ ë°›ìŒ. "
                f"ì‹¤í–‰ ì¤‘ì¸ íƒœìŠ¤í¬ {len(running_tasks)}ê°œ ì •ë¦¬ ì¤‘..."
            )

            # ëª¨ë“  ì‹¤í–‰ ì¤‘ì¸ ë³‘ë ¬ íƒœìŠ¤í¬ ì·¨ì†Œ
            for task in running_tasks:
                if not task.done():
                    task.cancel()

            # ì·¨ì†Œëœ íƒœìŠ¤í¬ ëŒ€ê¸° (ì •ë¦¬)
            if running_tasks:
                await asyncio.gather(*running_tasks, return_exceptions=True)

            logger.info(f"[{session_id}] ëª¨ë“  íƒœìŠ¤í¬ ì •ë¦¬ ì™„ë£Œ")

            # ì·¨ì†Œ ì´ë²¤íŠ¸ ìƒì„±
            cancel_event = WorkflowNodeExecutionEvent(
                event_type="workflow_cancelled",
                node_id="",
                data={"message": "ì›Œí¬í”Œë¡œìš°ê°€ ì·¨ì†Œë˜ì—ˆìŠµë‹ˆë‹¤"},
                timestamp=datetime.now().isoformat(),
            )
            yield cancel_event

            # CancelledError ì¬ë°œìƒ (ìƒìœ„ í˜¸ì¶œìì—ê²Œ ì „íŒŒ)
            raise

        finally:
            # ì„¸ì…˜ë³„ íŒŒì¼ í•¸ë“¤ëŸ¬ ì œê±° (ë©”ëª¨ë¦¬ ëˆ„ìˆ˜ ë°©ì§€)
            remove_session_file_handlers(session_id)
